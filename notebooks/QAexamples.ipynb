{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d743cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_29264\\3322886646.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import csv\n",
    "import util\n",
    "from transformers import DistilBertTokenizerFast, AutoTokenizer\n",
    "from transformers import DistilBertForQuestionAnswering, AutoModelForQuestionAnswering\n",
    "from transformers import AdamW\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from args import get_train_test_args\n",
    "from train import prepare_eval_data, prepare_train_data\n",
    "from util import compute_f1, compute_em\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfea286",
   "metadata": {},
   "source": [
    "### 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cad6441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_process(tokenizer, dataset_dict, dir_name, dataset_name, split):\n",
    "    #TODO: cache this if possible\n",
    "    cache_path = f'{dir_name}/{dataset_name}_encodings.pt'\n",
    "    if os.path.exists(cache_path) and not True:\n",
    "        tokenized_examples = util.load_pickle(cache_path)\n",
    "    else:\n",
    "        if split=='train':\n",
    "            tokenized_examples = prepare_train_data(dataset_dict, tokenizer)\n",
    "        else:\n",
    "            tokenized_examples = prepare_eval_data(dataset_dict, tokenizer)\n",
    "        util.save_pickle(tokenized_examples, cache_path)\n",
    "    return tokenized_examples\n",
    "\n",
    "def get_dataset(datasets, data_dir, tokenizer, split_name):\n",
    "    datasets = datasets.split(',')\n",
    "    dataset_dict = None\n",
    "    dataset_name=''\n",
    "    for dataset in datasets:\n",
    "        dataset_name += f'_{dataset}'\n",
    "        dataset_dict_curr = util.read_squad(f'{data_dir}/{dataset}')\n",
    "        dataset_dict = util.merge(dataset_dict, dataset_dict_curr)\n",
    "    data_encodings = read_and_process(tokenizer, dataset_dict, data_dir, dataset_name, split_name)\n",
    "    return util.QADataset(data_encodings, train=(split_name=='train')), dataset_dict\n",
    "\n",
    "def evaluate(model, data_loader, data_dict, return_preds=False, split='validation'):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model.eval()\n",
    "    pred_dict = {}\n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "    with torch.no_grad(), \\\n",
    "            tqdm(total=len(data_loader.dataset)) as progress_bar:\n",
    "        for batch in data_loader:\n",
    "            # Setup for forward\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = len(input_ids)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # Forward\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "            # TODO: compute loss\n",
    "\n",
    "            all_start_logits.append(start_logits)\n",
    "            all_end_logits.append(end_logits)\n",
    "            progress_bar.update(batch_size)\n",
    "\n",
    "    # Get F1 and EM scores\n",
    "    start_logits = torch.cat(all_start_logits).cpu().numpy()\n",
    "    end_logits = torch.cat(all_end_logits).cpu().numpy()\n",
    "    preds = util.postprocess_qa_predictions(data_dict,\n",
    "                                             data_loader.dataset.encodings,\n",
    "                                             (start_logits, end_logits))\n",
    "    preds = util.postprocess_qa_predictions(data_dict,\n",
    "                                                 data_loader.dataset.encodings,\n",
    "                                                 (start_logits, end_logits))\n",
    "    if split == 'validation':\n",
    "        results = util.eval_dicts(data_dict, preds)\n",
    "        results_list = [('F1', results['F1']),\n",
    "                        ('EM', results['EM'])]\n",
    "    else:\n",
    "        results_list = [('F1', -1.0),\n",
    "                        ('EM', -1.0)]\n",
    "    results = OrderedDict(results_list)\n",
    "    if return_preds:\n",
    "        return preds, results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7350d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir = 'datasets/indomain_val'\n",
    "val_datasets = 'squad,nat_questions,newsqa'\n",
    "\n",
    "eval_dir = 'datasets/oodomain_val'\n",
    "eval_datasets = 'race,relation_extraction,duorc'\n",
    "\n",
    "save_dir = 'save/'\n",
    "sub_file = 'mtl_submission_val.csv'\n",
    "\n",
    "model_dir = 'save/'\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523b8116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 721/721 [00:00<00:00, 27724.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 721/721 [00:03<00:00, 188.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 382/382 [00:00<00:00, 2364.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 382/382 [00:00<00:00, 2343.05it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/tinybert-6l-768d-squad2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/tinybert-6l-768d-squad2\")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "split_name = 'test' if 'test' in eval_dir else 'validation'\n",
    "\n",
    "log = util.get_logger(save_dir, f'log_{split_name}')\n",
    "\n",
    "checkpoint_path = os.path.join(model_dir, 'checkpoint')\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint_path)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "eval_dataset, eval_dict = get_dataset(eval_datasets, eval_dir, tokenizer, split_name)\n",
    "\n",
    "eval_loader = DataLoader(eval_dataset,batch_size=batch_size,sampler=SequentialSampler(eval_dataset))\n",
    "\n",
    "eval_preds, eval_scores = evaluate(model, eval_loader,eval_dict, return_preds=True, split=split_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a9a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_dict = {k:v for k,v in zip(eval_dict['id'], eval_dict['question'])}\n",
    "cont_dict = {k:v for k,v in zip(eval_dict['id'], eval_dict['context'])}\n",
    "answ_dict = {k:v for k,v in zip(eval_dict['id'], eval_dict['answer'])}\n",
    "\n",
    "f1s = []\n",
    "ems = []\n",
    "uuids = []\n",
    "\n",
    "for uuid in sorted(eval_preds):\n",
    "    \n",
    "    label = answ_dict[uuid]['text'][0]\n",
    "    pred = eval_preds[uuid]\n",
    "    \n",
    "    f1 = compute_f1(label, pred)\n",
    "    em = compute_em(label, pred)\n",
    "    \n",
    "    f1s.append(f1)\n",
    "    ems.append(em)\n",
    "    uuids.append(uuid)\n",
    "    \n",
    "eval_scores_2 = {k:[v1,v2] for k, v1, v2 in zip(uuids, f1s, ems)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6431fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.24.22 16:00:55] Eval F1: 50.66, EM: 35.34\n",
      "[11.24.22 16:00:55] Writing submission file to save/02.finetune/tinybert-only_classifier/baseline-01/validation_mtl_submission_val.csv...\n"
     ]
    }
   ],
   "source": [
    "results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in eval_scores.items())\n",
    "log.info(f'Eval {results_str}')\n",
    "# Write submission file\n",
    "sub_path = os.path.join(save_dir, split_name + '_' + sub_file)\n",
    "log.info(f'Writing submission file to {sub_path}...')\n",
    "with open(sub_path, 'w', newline='', encoding='utf-8') as csv_fh:\n",
    "    csv_writer = csv.writer(csv_fh, delimiter=',')\n",
    "    csv_writer.writerow(['Id', 'question', 'context', 'answer', 'Predicted', 'F1', 'EM'])\n",
    "    for uuid in sorted(eval_preds):\n",
    "        csv_writer.writerow([uuid, ques_dict[uuid], cont_dict[uuid], answ_dict[uuid]['text'][0], eval_preds[uuid], eval_scores_2[uuid][0],  eval_scores_2[uuid][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e49e746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('F1', 50.661239008093595), ('EM', 35.340314136125656)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
